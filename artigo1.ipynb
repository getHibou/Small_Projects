{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d32faf66",
      "metadata": {
        "id": "d32faf66"
      },
      "source": [
        "*Library and Installation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba06ea5",
      "metadata": {
        "id": "fba06ea5"
      },
      "outputs": [],
      "source": [
        "%pip install unsloth\n",
        "%pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73422c8e",
      "metadata": {
        "id": "73422c8e"
      },
      "source": [
        "*Initial Structure*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil"
      ],
      "metadata": {
        "id": "SCqmD4PlKgxt"
      },
      "id": "SCqmD4PlKgxt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3807fb",
      "metadata": {
        "id": "ee3807fb"
      },
      "outputs": [],
      "source": [
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ebd0ef",
      "metadata": {
        "id": "74ebd0ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# GPU INFORMATION\n",
        "# ---------------------------\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MODEL LIST\n",
        "# ---------------------------\n",
        "used_models = [\n",
        "    \"unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-1.7B-Base-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-4B-Base-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-Base-unsloth-bnb-4bit\",\n",
        "]\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MODEL SELECTION\n",
        "# ---------------------------\n",
        "model_name = \"unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit\"\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MODEL LOADING\n",
        "# ---------------------------\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7c2559",
      "metadata": {
        "id": "9b7c2559"
      },
      "outputs": [],
      "source": [
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef05c686",
      "metadata": {
        "id": "ef05c686"
      },
      "outputs": [],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\",\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd9d73fd",
      "metadata": {
        "id": "bd9d73fd"
      },
      "source": [
        "*Data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbaf3b7e",
      "metadata": {
        "id": "fbaf3b7e"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/Boamente_Treino_Atualizado.csv\")\n",
        "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "print(len(train_df))\n",
        "\n",
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9869bb90",
      "metadata": {
        "id": "9869bb90"
      },
      "source": [
        "*Prompt and Data Collator*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ca0067",
      "metadata": {
        "id": "b8ca0067"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "prompt = \"\"\"Aqui estÃ¡ uma sentenÃ§a.\n",
        "{}\n",
        "\n",
        "Se a sentenÃ§a contÃ©m ideaÃ§Ã£o suicida, responda \"Sim\". Caso contrÃ¡rio, responda \"NÃ£o\":\n",
        "\n",
        "A resposta correta Ã©: \"{}\"\"\"\n",
        "\n",
        "positivelabel = \"Sim\" #Yes\n",
        "negativelabel = \"NÃ£o\" #No\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        text_ = dataset_['text'].iloc[i]\n",
        "        label_ = dataset_['label'].iloc[i] #the csv is setup so that the label column corresponds exactly to the 3 classes defined above in the prompt (important)\n",
        "\n",
        "        text = prompt.format(text_, label_)\n",
        "\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "#apply formatting_prompts_func to train_df\n",
        "train_df['text'] = formatting_prompts_func(train_df)\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cea6e0e",
      "metadata": {
        "id": "6cea6e0e"
      },
      "outputs": [],
      "source": [
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7857248",
      "metadata": {
        "id": "b7857248"
      },
      "source": [
        "*Training with Cross-Validation*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psutil.cpu_count()\n",
        "import psutil"
      ],
      "metadata": {
        "id": "s4rRVXKHKB4f"
      },
      "id": "s4rRVXKHKB4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# IMPORTS (ORDEM IMPORTA!)\n",
        "# =====================================\n",
        "import psutil  # ðŸ”¥ OBRIGATÃ“RIO PARA UNSLOTH\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "from datasets import Dataset\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# CONFIG\n",
        "# =====================================\n",
        "NUM_FOLDS = 5\n",
        "SEED = 3407\n",
        "OUTPUT_DIR = \"/content/roc_cv_results\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# DATASET\n",
        "# =====================================\n",
        "full_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "\n",
        "skf = StratifiedKFold(\n",
        "    n_splits=NUM_FOLDS,\n",
        "    shuffle=True,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# STORAGE\n",
        "# =====================================\n",
        "fold_results = []\n",
        "roc_curves = []   # (fpr, tpr, auc)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# CROSS-VALIDATION\n",
        "# =====================================\n",
        "for fold, (train_idx, val_idx) in enumerate(\n",
        "    skf.split(full_dataset[\"text\"], full_dataset[\"label\"])\n",
        "):\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting Fold {fold+1}/{NUM_FOLDS}\")\n",
        "\n",
        "    train_dataset = full_dataset.select(train_idx)\n",
        "    val_dataset   = full_dataset.select(val_idx)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=SEED,\n",
        "        output_dir=f\"/content/outputs/fold_{fold+1}\",\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=True,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=training_args,\n",
        "        formatting_func=formatting_prompts_func,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # TRAIN\n",
        "    # -----------------------------\n",
        "    trainer.train()\n",
        "\n",
        "    # -----------------------------\n",
        "    # PREDICT\n",
        "    # -----------------------------\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    logits = predictions.predictions\n",
        "\n",
        "    probs = softmax(logits, axis=1)[:, 1]\n",
        "    y_true = np.array(val_dataset[\"label\"])\n",
        "\n",
        "    # -----------------------------\n",
        "    # AUC + ROC\n",
        "    # -----------------------------\n",
        "    auc = roc_auc_score(y_true, probs)\n",
        "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
        "\n",
        "    # -----------------------------\n",
        "    # SAVE PER FOLD\n",
        "    # -----------------------------\n",
        "    np.save(f\"{OUTPUT_DIR}/y_true_fold_{fold+1}.npy\", y_true)\n",
        "    np.save(f\"{OUTPUT_DIR}/y_score_fold_{fold+1}.npy\", probs)\n",
        "    np.save(f\"{OUTPUT_DIR}/fpr_fold_{fold+1}.npy\", fpr)\n",
        "    np.save(f\"{OUTPUT_DIR}/tpr_fold_{fold+1}.npy\", tpr)\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold + 1,\n",
        "        \"auc\": auc\n",
        "    })\n",
        "\n",
        "    roc_curves.append((fpr, tpr, auc))\n",
        "\n",
        "    print(f\"ðŸ“Š Fold {fold+1} | AUC = {auc:.4f}\")\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# RESULTS TABLE\n",
        "# =====================================\n",
        "df_results = pd.DataFrame(fold_results)\n",
        "df_results.to_csv(f\"{OUTPUT_DIR}/auc_per_fold.csv\", index=False)\n",
        "\n",
        "mean_auc = df_results[\"auc\"].mean()\n",
        "std_auc  = df_results[\"auc\"].std()\n",
        "\n",
        "print(\"\\nðŸ“Œ AUC por fold:\")\n",
        "display(df_results)\n",
        "\n",
        "print(f\"\\nâœ… Mean AUC: {mean_auc:.4f} Â± {std_auc:.4f}\")\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# MEAN ROC CURVE\n",
        "# =====================================\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "tprs = []\n",
        "\n",
        "for fpr, tpr, _ in roc_curves:\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "\n",
        "np.save(f\"{OUTPUT_DIR}/mean_fpr.npy\", mean_fpr)\n",
        "np.save(f\"{OUTPUT_DIR}/mean_tpr.npy\", mean_tpr)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# PLOT\n",
        "# =====================================\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(\n",
        "    mean_fpr,\n",
        "    mean_tpr,\n",
        "    linewidth=2,\n",
        "    label=f\"Mean ROC (AUC = {mean_auc:.4f} Â± {std_auc:.4f})\"\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Mean ROC Curve â€“ Stratified 5-Fold CV\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6hLxlr_FCQDs"
      },
      "id": "6hLxlr_FCQDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil"
      ],
      "metadata": {
        "id": "1stDXz7PkFTN"
      },
      "id": "1stDXz7PkFTN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "num_folds = 5\n",
        "batch_size = 64\n",
        "seed = 3407\n",
        "\n",
        "output_dir = \"/home/joaopedro/joaopedro/llm/roc\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Dataset completo (jÃ¡ formatado com prompt)\n",
        "full_dataset = Dataset.from_pandas(data, preserve_index=False)\n",
        "\n",
        "skf = StratifiedKFold(\n",
        "    n_splits=num_folds,\n",
        "    shuffle=True,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "all_fold_aucs = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(\n",
        "    skf.split(full_dataset[\"text\"], full_dataset[\"label\"])\n",
        "):\n",
        "    print(f\"\\nðŸš€ Starting Fold {fold + 1}/{num_folds}\")\n",
        "\n",
        "    train_dataset = full_dataset.select(train_idx)\n",
        "    val_dataset   = full_dataset.select(val_idx)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=seed,\n",
        "        output_dir=f\"outputs/fold_{fold}\",\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=True,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=training_args,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # ===== InferÃªncia =====\n",
        "    val_texts  = val_dataset[\"text\"]\n",
        "    val_labels = val_dataset[\"label\"]\n",
        "\n",
        "    fold_probabilities = []\n",
        "    fold_labels = []\n",
        "\n",
        "    tokenized_inputs = []\n",
        "    for text, label in zip(val_texts, val_labels):\n",
        "        # remove resposta do final\n",
        "        test_str = text.rsplit(\"A resposta correta Ã©:\", 1)[0] + \"A resposta correta Ã©:\"\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            test_str,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "        tokenized_inputs.append((tokenized, label))\n",
        "\n",
        "    # ordenar por comprimento\n",
        "    tokenized_inputs.sort(\n",
        "        key=lambda x: x[0][\"input_ids\"].shape[1]\n",
        "    )\n",
        "\n",
        "    grouped_inputs = defaultdict(list)\n",
        "    for tokenized, label in tokenized_inputs:\n",
        "        length = tokenized[\"input_ids\"].shape[1]\n",
        "        grouped_inputs[length].append((tokenized, label))\n",
        "\n",
        "    for length, group in tqdm(grouped_inputs.items()):\n",
        "        for i in range(0, len(group), batch_size):\n",
        "            batch = group[i:i + batch_size]\n",
        "\n",
        "            input_ids = torch.cat(\n",
        "                [item[0][\"input_ids\"] for item in batch]\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            attention_mask = torch.cat(\n",
        "                [item[0][\"attention_mask\"] for item in batch]\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            labels = [item[1] for item in batch]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits[:, -1, :2]\n",
        "            probs = F.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "\n",
        "            fold_probabilities.extend(probs)\n",
        "            fold_labels.extend(labels)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(fold_labels, fold_probabilities)\n",
        "    fold_auc = auc(fpr, tpr)\n",
        "    all_fold_aucs.append(fold_auc)\n",
        "\n",
        "    np.savetxt(os.path.join(output_dir, f\"fpr_modelX_fold{fold}.txt\"), fpr)\n",
        "    np.savetxt(os.path.join(output_dir, f\"tpr_modelX_fold{fold}.txt\"), tpr)\n",
        "\n",
        "    with open(os.path.join(output_dir, f\"auc_modelX_fold{fold}.txt\"), \"w\") as f:\n",
        "        f.write(f\"{fold_auc:.6f}\\n\")\n",
        "\n",
        "    print(f\"ðŸ“Š Fold {fold + 1} â€” AUC: {fold_auc:.4f}\")\n",
        "\n",
        "# ===== Resultados finais =====\n",
        "mean_auc = np.mean(all_fold_aucs)\n",
        "std_auc  = np.std(all_fold_aucs)\n",
        "\n",
        "print(\"\\nâœ… Final Cross-Validation Results\")\n",
        "print(f\"Mean AUC: {mean_auc:.4f}\")\n",
        "print(f\"Std AUC:  {std_auc:.4f}\")\n",
        "\n",
        "with open(os.path.join(output_dir, \"auc_summary_modelX.txt\"), \"w\") as f:\n",
        "    f.write(f\"Mean AUC: {mean_auc:.6f}\\n\")\n",
        "    f.write(f\"Std AUC:  {std_auc:.6f}\\n\")"
      ],
      "metadata": {
        "id": "woIo_qioiXnY"
      },
      "id": "woIo_qioiXnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89899d51",
      "metadata": {
        "id": "89899d51"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from datasets import Dataset\n",
        "\n",
        "#Number of folds for cross-validation\n",
        "num_folds = 5\n",
        "\n",
        "#Convert the pandas dataset to the Hugging Face dataset format (if not already done)\n",
        "full_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "\n",
        "#Create fold indices for cross-validation\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=3407)\n",
        "\n",
        "#Store metrics from each fold\n",
        "all_results = []\n",
        "\n",
        "#Iterate over folds\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(full_dataset[\"text\"], full_dataset[\"label\"])):\n",
        "    print(f\"\\nðŸš€ Starting Fold {fold+1}/{num_folds}...\")\n",
        "\n",
        "    #Create training and validation subsets\n",
        "    train_dataset = full_dataset.select(train_idx)\n",
        "    val_dataset = full_dataset.select(val_idx)\n",
        "\n",
        "    #Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=f\"outputs/fold_{fold+1}\",  #Separate directory for each fold\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",  #Disable logging to external services\n",
        "        group_by_length=True,\n",
        "    )\n",
        "\n",
        "    #Create the Trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,  #Adding validation\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=training_args,\n",
        "        formatting_func=formatting_prompts_func,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    #Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    #Evaluate the model on this fold's validation set\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"ðŸ“Š Results for Fold {fold+1}: {metrics}\")\n",
        "\n",
        "    # Store the metrics\n",
        "    all_results.append(metrics)\n",
        "\n",
        "#Compute the mean metrics across all folds\n",
        "final_results = {\n",
        "    metric: np.mean([result[metric] for result in all_results]) for metric in all_results[0]\n",
        "}\n",
        "\n",
        "print(\"\\nâœ… Final average metrics after cross-validation:\")\n",
        "print(final_results)\n",
        "\n",
        "#Note: the output is in Portuguese because it was the old version of the code. In the new version, the code and comments are in English."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62915c1c",
      "metadata": {
        "id": "62915c1c"
      },
      "source": [
        "*Training with 80/20 (Hold-out)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fccdd4",
      "metadata": {
        "id": "40fccdd4"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dafeffd6",
      "metadata": {
        "id": "dafeffd6"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872c0cba",
      "metadata": {
        "id": "872c0cba"
      },
      "source": [
        "*Inference*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead285d6",
      "metadata": {
        "id": "ead285d6"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1a1b4c",
      "metadata": {
        "id": "ab1a1b4c"
      },
      "source": [
        "*Evaluation with metrics*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5179be8d",
      "metadata": {
        "id": "5179be8d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "#Define output directory\n",
        "output_dir = \"/home/joaopedro/joaopedro/llm/roc\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#Load validation data\n",
        "val_texts = val_df['text'].tolist()\n",
        "val_labels = val_df['label'].tolist()\n",
        "\n",
        "#Define storage\n",
        "all_probabilities = []\n",
        "all_labels = []\n",
        "\n",
        "#Step 1: Tokenization and sorting by token length\n",
        "tokenized_inputs = []\n",
        "for text, label in zip(val_texts, val_labels):\n",
        "    test_str = prompt.format(text, \"\")\n",
        "    tokenized_input = tokenizer(test_str, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    tokenized_inputs.append((tokenized_input, test_str, label))\n",
        "\n",
        "#Sort by tokenized length\n",
        "tokenized_inputs.sort(key=lambda x: x[0]['input_ids'].shape[1])\n",
        "\n",
        "#Step 2: Group by tokenized length\n",
        "grouped_inputs = defaultdict(list)\n",
        "for tokenized_input, test_str, label in tokenized_inputs:\n",
        "    length = tokenized_input['input_ids'].shape[1]\n",
        "    grouped_inputs[length].append((tokenized_input, test_str, label))\n",
        "\n",
        "#Step 3: Process batches\n",
        "batch_size = 64\n",
        "\n",
        "for length, group in tqdm(grouped_inputs.items()):\n",
        "    for i in range(0, len(group), batch_size):\n",
        "        batch = group[i:i + batch_size]\n",
        "        batch_inputs = [item[0] for item in batch]\n",
        "        batch_labels = [item[2] for item in batch]\n",
        "\n",
        "        #Concatenate batch inputs\n",
        "        input_ids = torch.cat([item['input_ids'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "        attention_mask = torch.cat([item['attention_mask'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "\n",
        "        #Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #Extract logits for classification\n",
        "        logits = outputs.logits[:, -1, :2]\n",
        "        probabilities = F.softmax(logits, dim=-1)[:, 1].cpu().numpy()  #Probability of class 1\n",
        "\n",
        "        #Store results\n",
        "        all_probabilities.extend(probabilities)\n",
        "        all_labels.extend(batch_labels)\n",
        "\n",
        "#Step 4: Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#Save ROC data\n",
        "np.savetxt(os.path.join(output_dir, \"fpr_Qwen3_14B.txt\"), fpr)\n",
        "np.savetxt(os.path.join(output_dir, \"tpr_Qwen3_14B.txt\"), tpr)\n",
        "np.savetxt(os.path.join(output_dir, \"thresholds_Qwen3_14B.txt\"), thresholds)\n",
        "with open(os.path.join(output_dir, \"auc_Qwen3_14B.txt\"), \"w\") as f:\n",
        "    f.write(f\"AUC: {roc_auc:.4f}\\n\")\n",
        "\n",
        "#Step 5: Find optimal threshold\n",
        "optimal_idx = (tpr - fpr).argmax()\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "#Step 6: Convert probabilities to binary predictions\n",
        "all_outputs = (all_probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "#Step 7: Compute evaluation metrics\n",
        "cm = confusion_matrix(all_labels, all_outputs)\n",
        "accuracy = accuracy_score(all_labels, all_outputs)\n",
        "precision = precision_score(all_labels, all_outputs)\n",
        "recall = recall_score(all_labels, all_outputs)\n",
        "f1 = f1_score(all_labels, all_outputs)\n",
        "\n",
        "#Step 8: Save all metrics + confusion matrix to a single file\n",
        "metrics_file = os.path.join(output_dir, \"metrics_Qwen3_14B.txt\")\n",
        "with open(metrics_file, \"w\") as f:\n",
        "    f.write(\"===== Model Evaluation Metrics =====\\n\")\n",
        "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
        "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
        "    f.write(f\"Recall: {recall:.4f}\\n\")\n",
        "    f.write(f\"F1-Score: {f1:.4f}\\n\")\n",
        "    f.write(f\"AUC: {roc_auc:.4f}\\n\")\n",
        "\n",
        "    f.write(\"\\n===== Confusion Matrix =====\\n\")\n",
        "    np.savetxt(f, cm, fmt=\"%d\")\n",
        "\n",
        "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
        "\n",
        "#Step 9: Save confusion matrix separately\n",
        "np.savetxt(os.path.join(output_dir, \"confusion_matrix_Qwen3_14B.txt\"), cm, fmt='%d')\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}